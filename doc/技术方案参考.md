# 技术方案参考

## 1. ATAK空搜地救系统

0602 阅读：

1.  [ATAK Integration through ROS for Autonomous Air-ground Team](https://ieeexplore.ieee.org/abstract/document/9476676/)

   > 内容概述：
   >
   > ATAK 手机app派出一架飞行器在一个区域搜索受困人员。飞行器检测到可能受害者的位置后，反馈到ATAK app上。然后，救援人员可以将地面机器人分配到指定位置，来进一步帮助受困者，并在需要时提供食物、水和急救。

   > 亮点：
   >
   > 自主机器人团队由四旋翼和无人驾驶地面车辆组成。机器人团队的人机界面是 Android Team Awareness Kit (ATAK)，
   >
   > **启发是人机交互界面。**
   >
   > 具体没有协同建图的内容。主要是空中搜索、地面救援（运输），人机交互。

   > 软硬件实现：
   >
   > - 空中：
   >
   >   1. 650 四轴飞行器框架.
   >
   >   2. 运行 PX4 固件的 Hex Cube Black 飞行控制器，机外模式下运行，从Intel NUC 接收导航命令。
   >
   >   3. Intel深度摄像头 D435i 和 T265。
   >
   >      1. 视觉惯性里程计 (VIO) 算法使用 T265，
   >      2. D435i 为对象分类算法以及对象检测和定位系统提供 RGB 图像。
   >
   >   4.  NUC 的内部 WiFi 适配器与实验室网络进行通信。
   >
   >      ![图 4.-UAS 框图。](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9476247/9476672/9476676/9476676-fig-4-source-small.gif)
   >
   > - 地面：
   >
   >   1. NVIDIA T4 作为GPU，重点突出边缘计算。
   >
   >      目前我们已有的是Intel工控机。
   >
   >   2. 整体机械架构更倾向于运载。ClearPath Husky成熟结构。
   >
   >   3. 加速度计和陀螺仪：MicroSrain INS 。无GPS。
   >
   >   4.  成像激光雷达：Ouster OS1-64 。
   >
   >      提供 3D 点云，用于映射、生成避障成本图以及计算检测到的物体相对于 UGS 的位置。
   >
   >   5.  D435i 在雷达下方。
   >
   >      该相机提供目标检测算法图像和可以与 Ouster 点云融合的点云。
   >
   >   6. 还安装了两个额外的 FLIR Blackfly 相机。
   >
   >      ![图 6.-UGS 框图。](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9476247/9476672/9476676/9476676-fig-6-source-small.gif)
   >
   > - 通信
   >
   >   主要是atak_bridge（开源ROS结点）。
   >
   >   ![图 8. - 软件框图。](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/9476247/9476672/9476676/9476676-fig-8-source-small.gif)
   >
   > - 无人机软件
   >
   >   ARL 的 Mav 平台软件，利用 PX4、ROS 和 MAVROS 开发，可在仿真和机器人平台之间移植。

   

## 2. UAV-UGV协同系统

阅读[[Energy-Efficient Ground Traversability Mapping Based on UAV-UGV Collaborative System](https://ieeexplore.ieee.org/abstract/document/9521583/)](https://www.worldscientific.com/doi/abs/10.1142/S2301385021410065)

这一篇主要偏向于地图构建和可达性的分析。对于融合建图，提出的是MGPMF系统，分为图像匹配、地图融合、主动感知三部分。

效果是可以从航空**图像**和地面**激光雷达**地图中采样的稀疏数据点构建地图。

![图 5. - (a) 无人机在某个点拍摄的图像和 (b) CNN 处理的语义地图和 (c) SLAM 在这一点上的激光雷达地图和 (d) 融合地图。](https://ieeexplore.ieee.org/mediastore_new/IEEE/content/media/7511293/9714516/9521583/chen5abcd-3107291-small.gif)

图中a为无人机图像，b为CNN处理的语义地图，c为激光雷达地图，d为融合地图。

我觉得做的不是很出色，三维激光雷达或许就能做到相近的效果。

## 3. 视觉抓取1-深度库

 https://github.com/zzrs123/deep_grasp_demo

这个包可以使用深度学习方法，在movelt执行抓取。软件环境为Ubuntu1804和ROS melodic。现在包含了深度图像中采样抓取和3D点云中采样抓取两种模式。

## 4. 抓取[tensor2robot](https://github.com/google-research/tensor2robot)

论文网站：

https://sites.google.com/site/grasp2vec/localization

新闻：

https://www.linkresearcher.com/theses/d23377c3-fc9b-4a9c-89e1-9aa615b5669f

代码：

https://github.com/google-research/tensor2robot/tree/master/research/grasp2vec

谷歌的机械臂开源包，可让机械臂自学抓取物体。不标注物体。
